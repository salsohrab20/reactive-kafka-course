## üì• Kafka Consumer: Pull-Based Model Explained

This section explains how the Kafka **consumer** works internally, specifically focusing on how messages are retrieved from a Kafka topic.

---

### üîÑ Is Kafka Consumer Push or Pull?

Kafka consumers follow a **pull-based model**.

- ‚úÖ **Kafka does NOT push messages to consumers.**
- ‚úÖ **Consumers explicitly pull (or poll) messages from Kafka.**

---

### ‚ùì Why Pull-Based?

You might wonder:
> *"Since the consumer is running, why can‚Äôt Kafka just push messages to it?"*

Here's why Kafka uses a pull-based approach:

- Kafka is designed for **high throughput** and **low latency**.
- It can easily handle **millions of messages per second**.
- Producers can write data much faster than consumers may be able to process.

üìå **Kafka has no way of knowing the speed or capacity of the consumer.**  
If it pushed messages to the consumer directly, it could easily **overwhelm** slower consumers.

By letting consumers **pull at their own pace**, Kafka ensures:

- Backpressure is managed.
- Consumers stay in control of how much data they receive.
- Overall system stability is maintained.

---

### ‚öôÔ∏è How Polling Works

The consumer repeatedly polls Kafka for new messages:

1. "Can you give me some messages?"
2. Kafka responds with the requested number of records (up to a configured limit).
3. Consumer processes them.
4. Then it polls again for more.

This polling loop is continuous and **controlled by the consumer**.

---

### üîß Tuning with `max.poll.records`

Kafka allows you to control how many messages a consumer should receive per poll using the `max.poll.records` configuration.

- **Default**: `500`
- **Configurable**: You can increase or decrease this based on your consumer‚Äôs capacity.

### properties
# In consumer properties 
-  max.poll.records=500 

```This gives you fine-grained control over: How many records are fetched in one request. How much work the consumer handles in each poll cycle. ```

### üß† Similarity to Reactive Streams
This is conceptually similar to Reactive Streams and Publisher-Subscriber patterns.
A subscriber must explicitly request (n) items from the publisher.
Similarly, a Kafka consumer must poll for messages from Kafka.

#### This model ensures:
 - Controlled flow of data 
 - Avoidance of memory overflow or processing overload

### ‚úÖ Summary
Kafka consumers pull data ‚Äî Kafka does not push messages.
This design supports scalability, backpressure, and fault-tolerance.
The number of messages per poll is controlled via max.poll.records.
The consumer decides when and how much data it wants to receive.

üí° Pro Tip: Tune max.poll.records and other consumer settings based on your application's processing speed and latency requirements.

---

### **Kafka Consumer Groups Demonstration Summary:**

1. **Two Consumers from Different Groups:**

    * One consumer is started from the **Payment Service (PS)** group.
    * Another consumer is started from the **Inventory Service** group.
    * A producer sends messages.
    * **Result:** Both consumers (from different groups) receive **all messages** because Kafka delivers messages to **each group independently**.

2. **Adding a Second Consumer to the Same Group (Inventory Service):**

    * A new consumer is added to the **Inventory Service** group.
    * Now, Inventory Service group has **two consumers**.
    * When a message is sent:

        * **Only one of the two consumers** in the Inventory Service group receives it.
        * This is due to **load balancing** within a consumer group: Kafka distributes messages across consumers **in the same group**.

3. **Observed Issue:**

    * One consumer in the group appears idle.
    * Kafka is distributing messages, but not all consumers are getting messages because of **partition assignment** rules.
    * **Solution hint:** Use **partitions** to enable parallel processing and better distribution.

4. **Console Consumer Behavior Without Explicit Group:**

    * When using Kafka console consumers **without specifying a group**, each instance **automatically gets its own consumer group**.
    * As a result, **each console consumer gets all the messages**, because they are treated as separate groups.

---
### Scenario 1: Consumers in Different Groups (All receive messages)

```Topic: orders
+-----------------+
|     Producer    |
+--------+--------+
|
Sends Messages
|
------------------------------------------
|                                        |
+----v----+                              +----v----+
|Consumer | (Group: Payment Service - PS)|Consumer | (Group: Inventory Service - IS)
+---------+                              +---------+
```
‚Üí Both consumers receive ALL messages independently.

### üî∏ Scenario 2: Multiple Consumers in the Same Group (Load Balanced)
```
Topic: orders (1 Partition for simplicity)
+-----------------+
|     Producer    |
+--------+--------+
|
Sends Messages
|
v
Group: Inventory Service (IS)
+----------------+    +----------------+
|   Consumer 1   |    |   Consumer 2   |
+----------------+    +----------------+
```

‚Üí Kafka assigns the partition to only ONE consumer at a time.
‚Üí Only one of them gets each message.
‚Üí The other may stay idle.

### üî∏ Scenario 3: Console Consumers without Group (Each Gets Own Group)

```Topic: orders
+-----------------+
|     Producer    |
+--------+--------+
|
Sends Messages
|
------------------------------------------
|                                        |
+----v----+ (Group: **ConsoleConsumer**#1)  +-----v----+ (Group: **ConsoleConsumer**#2)
|Consumer |                             |Consumer  |
+---------+                             +----------+
```
‚Üí Kafka sees them as different groups.
‚Üí Each consumer receives ALL messages.

### ‚úÖ Key Takeaways:
- Different groups: All get the same messages (good for independent services). 
- Same group: Kafka load-balances messages (good for scaling within one service). 
- No group (console): Treated as unique groups ‚Üí all messages received by each.

# üìö Kafka Scenarios: Partition Count vs. Consumer Scaling

This section covers real-world Kafka behavior and edge cases related to **consumer scaling**, **partition reassignment**, and **dynamic partition changes**.

---

## üîÅ Scenario 1: Scaling Consumers in a Consumer Group

### üß© Setup

- 1 **topic** with **3 partitions**
- 1 **consumer group**
- Initially, **1 consumer**

### Behavior

| Consumers in Group | Kafka Behavior                             |
|--------------------|---------------------------------------------|
| 1                  | All 3 partitions assigned to the consumer   |
| 2                  | 1 partition reassigned to the new consumer  |
| 3                  | 1 partition per consumer (balanced)         |
| 4+                 | Extra consumers remain **idle**             |

> ‚ö†Ô∏è **Kafka does not assign a single partition to multiple consumers**, to maintain **message ordering**.

### üí° Rule of Thumb

> ‚úÖ **Max number of active consumers = number of partitions**

---

## üîÑ Scenario 2: Increasing Partition Count After Production

### Setup

- Initially, topic has **3 partitions**
- Messages are keyed (e.g., `key1`, `key2`, etc.) and mapped deterministically using Kafka's `murmur2` hash
- Messages are being produced and consumed correctly

### Problem

- You realize more throughput is needed, so you run a command to **increase partitions to 4**
- New messages may now get assigned to the **new 4th partition**

### Consequence

- This can **break message ordering** for a given key!
- Example: A message with `key=4` initially went to Partition 2, now starts going to Partition 3
- Older message (from original partition) might be processed **after** the new one ‚Üí **ordering violation**

---

## ‚ö†Ô∏è Important Caveats of Changing Partition Count

| Issue                            | Impact                         |
|----------------------------------|--------------------------------|
| Repartitioning after production  | Breaks message key ordering    |
| Consumers rebalance              | Partitions reassigned properly |
| Extra consumer after limit       | Stays idle                     |

---

## üõ†Ô∏è Solutions to Handle Partition Increase

### ‚úÖ 1. Design Upfront

- Plan the **number of partitions ahead of time**
- Avoid repartitioning after production has started

---

### ‚úÖ 2. Accept Short-Term Ordering Issues

- If message ordering is **not critical**, it's acceptable to:
   - Tolerate reordering for a short period
   - Let the producer continue sending to the topic

---

### ‚úÖ 3. Stop Producer ‚Üí Drain ‚Üí Restart

- Steps:
   1. **Stop the producer**
   2. Let consumers **drain** all current partitions
   3. Increase partition count
   4. Restart the producer

- Ensures clean transition and **preserves ordering**

---

### ‚úÖ 4. Create a New Topic

- Best option for **zero disruption** and **no ordering issues**
- Steps:
   1. Create a **new topic** with more partitions
   2. Update producer to write to the **new topic**
   3. Allow consumers to finish processing old topic
   4. Update consumers to consume from the **new topic**
   5. Delete the old topic once fully drained

---

## ‚úÖ Summary

- Kafka partitions determine how many consumers can be active in a group
- **Repartitioning after production begins can break ordering**
- There are **multiple strategies** to handle this:
   - Plan partitioning upfront
   - Stop the producer and drain
   - Accept short-term ordering trade-offs
   - Migrate to a new topic

> Choose a strategy based on **your application‚Äôs tolerance for message reordering and downtime**.


# üìä Kafka Monitoring: Tracking Consumer Lag & Offsets

Kafka provides a built-in command-line tool to track **how many messages have been produced**, **how many have been consumed**, and **how many are still pending (lagging)**.

This file walks through a live demo scenario using the `kafka-consumer-groups.sh` command.

---

## üß™ Demo Setup

- **Topic**: `hello-world` with **2 partitions**
- **Producer**: Starts producing messages with **different keys**
- **Consumer Group**: `kg`
    - Initially, the consumer is **not started**

---

## üßµ Step-by-Step Behavior

### 1Ô∏è‚É£ Produce Messages Before Consumer Starts

- Sent 8 messages to the topic before starting the consumer
- Because the consumer group `kg` does **not yet exist**, Kafka won‚Äôt track offsets yet

### 2Ô∏è‚É£ Start Consumer Group `kg`

- When the consumer starts for the first time:
    - It **does NOT consume old messages**
    - It only processes **new messages from this point forward**

> üìå This is the default behavior unless the offset reset policy is explicitly set (e.g., `--from-beginning`).

---

## üîç Using the Kafka Lag Monitoring Command

```bash
kafka-consumer-groups.sh \
  --bootstrap-server localhost:9092 \
  --group kg \
  --describe
```
--- 
### Output Example:
| TOPIC       | PARTITION | CURRENT-OFFSET | LOG-END-OFFSET | LAG | CONSUMER-ID |
|-------------|-----------|----------------|----------------|-----|--------------|
| hello-world | 0         | 4              | 4              | 0   | consumer-1   |
| hello-world | 1         | 4              | 4              | 0   | consumer-1   |


Current Offset: Last message the consumer has processed

Log End Offset: Last message that was produced to the partition

Lag = Messages still waiting to be processed = Log End Offset - Current Offset

### Stop the Consumer ‚Üí Produce More Messages
Produced additional messages while the consumer was offline

Re-ran the monitoring command:

Updated Output:

| PARTITION | CURRENT-OFFSET | LOG-END-OFFSET | LAG |
|-----------|----------------|----------------|-----|
| 0         | 6              | 7              | 1   |
| 1         | 6              | 11             | 5   |


üéØ Kafka tracks exactly how many messages are pending per partition for that group.

---
### ‚úÖ Summary

| Concept                | Description                                              |
| ---------------------- | -------------------------------------------------------- |
| `Current Offset`       | Last processed message offset per partition              |
| `Log End Offset`       | Latest message offset produced to the partition          |
| `Lag`                  | Messages waiting to be processed                         |
| Tool Used              | `kafka-consumer-groups.sh --describe`                    |
| Default Consumer Start | Begins from latest message (unless configured otherwise) |
| Offset Persistence     | Kafka remembers where each group left off                |
